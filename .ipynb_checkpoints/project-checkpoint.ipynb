{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eric Parsons, Brandon Lavigne, Jordan Rayfield\n",
    "\n",
    "* Note: Ensure that when you run the model, that you click on the window as it may start out minimized in the task bar. In addition, ensure that you run `pip install gym` and `pip install gym[atari]` to download the environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Models\n",
    "Below are models trained with the ddqn architecture. If you wish to change length that each network plays for, modify the seconds_to_play parameter. Each model is currently set to play for 5 seconds for a quick demonstration. The cell for each model can be run to load in a pretrained model and run the game, but there are also gifs provided below that demonstrate the same models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddqn.play as play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDQN Pong\n",
    "This model was trained for 3,000,000 frames on pong with a reward function that penalizes changes in momentum. The agent only moves when neccesary to hit the ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'pong'\n",
    "env = play.create_environment(game_name)\n",
    "play.play(env, game_name, seconds_to_play=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PongUrl](https://i.imgur.com/lTvAUV4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDQN Space Invaders\n",
    "This model was trained for 7,000,000 frames on space invaders. The model plays like a very skilled human player and you can see where it clearly makes attempts to shoot the mothership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'space_invaders'\n",
    "env = play.create_environment(game_name)\n",
    "play.play(env, game_name, seconds_to_play=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DDQNSpaceInvaders](https://i.imgur.com/IhbWQp1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDQN Breakout\n",
    "This model was trained for 7,000,000 frames on breakout. It plays quite well, but it needs more frames to fully converge. You can notice some instances of tunneling where it hits the ball behind the blocks in order to score rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'breakout'\n",
    "env = play.create_environment(game_name)\n",
    "play.play(env, game_name, seconds_to_play=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DDQNBreakout](https://i.imgur.com/DLVzyDs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C Models\n",
    "Running with the implemented A2C algorithm architecture plays a game session, and stopis it for you. Though for some games, the agent never loses, so to stop it, just press the `stop` button for this cell to interrupt the kernel. BUT NOTE THAT IF YOU DO THIS, YOU NEED TO RESTART THE KERNEL (tensorflow related issue). Below are gifs of the latest models, but feel free to run and modify the code to play the models natively.\n",
    "##### A2C Breakout,\n",
    "always manages to beat each generation. It realized that the most optimal path would be to dig a tunnel, as it almost always does. Near the end where they are little blocks left, it tries new things and sometimes loses a life. It's never a fatal mistake though.\n",
    "##### A2C Space Invaders,\n",
    "has amazing accuracy, but preforms similar to breakout when there's little enemies left.\n",
    "##### A2C Star Gunner,\n",
    "seems to never die. It always dodges incoming enemy bullets and plays with amazing reflexes. I've played this model on hours on end, and its never seems to lose a single life.\n",
    "##### A2C James Bond,\n",
    "has trouble getting far. This game took much to train to ensure it passes the volcanoe obsticle. It eventually passes it, and sometimes gets to the second level.\n",
    "##### A2C Pong,\n",
    "wins flawlessly every single time. Not only does it always win, but also manages to beat the opponent on a single swing, always.\n",
    "\n",
    "The amount of generations are listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakout 105,000|Space Invaders 820,000|Star Gunner 835,000|James Bond 800,000|Pong 1,515,000\n",
    "- | - | - | - | - |\n",
    "![A2CBreakout](https://i.imgur.com/4hyRox7.gif) | ![A2CSpaceInvaders](https://i.imgur.com/r8lyu6n.gif) | ![A2CStargunner](https://i.imgur.com/zmobGTs.gif) | ![A2CJamesBond](https://i.imgur.com/lvix1lW.gif) | ![A2CPong](https://i.imgur.com/2U0v5ly.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands below may used to play each custom model. Refer to `A2C Models` to stop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import a2c.play as play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play.main('BreakoutNoFrameskip-v4', 455000, 'a2c/models', 'a2c/gifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play.main('JamesbondNoFrameskip-v4', 1305000, 'a2c/models', 'a2c/gifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play.main('PongNoFrameskip-v4', 1515000, 'a2c/models', 'a2c/gifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play.main('SpaceInvadersNoFrameskip-v4', 1335000, 'a2c/models', 'a2c/gifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play.main('StarGunnerNoFrameskip-v4', 1360000, 'a2c/models', 'a2c/gifs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCPG Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the further-along implementation of the Monte Carlo Gradinet Policy. Although it did not finish converging, it has reached a peak win rate of 32%. This model will run for 10 seconds, and you can observe that it generally does a good job of trying to knock the ball in the other direction, even though it does not always catch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace\n",
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# hyperparameters\n",
    "resume = True  # resume from previous checkpoint?\n",
    "render = False\n",
    "MAX_FRAMES = 2000000\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class CnnPGN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnPGN, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2304, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        #print(\"Feeding forward:\")\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195]  # crop\n",
    "    I = I[::2, ::2, 0]  # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "    return I\n",
    "\n",
    "\n",
    "def select_action(policy, state, device):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    state = state.view((1, 1, 80, 80)).to(device)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def play_game():\n",
    "    render = True\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "\n",
    "    observation = env.reset()\n",
    "\n",
    "    previous_frame = None\n",
    "    episode_num = 0\n",
    "    frame_count = 1\n",
    "    current_frame = prepro(observation)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy = CnnPGN(current_frame.shape, 4).to(device)\n",
    "    policy.load_state_dict(torch.load('mcpg/policy_1550000_Final'))\n",
    "\n",
    "    print(\"Preparing Game:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while episode_num < 2:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        if time.time() - start_time >= 10:\n",
    "            env.close()\n",
    "            return\n",
    "        \n",
    "        time.sleep(.005)\n",
    "        difference_image = current_frame - \\\n",
    "            previous_frame if previous_frame is not None else np.zeros_like(\n",
    "                current_frame)\n",
    "        previous_frame = current_frame\n",
    "\n",
    "        action = select_action(policy, difference_image, device)\n",
    "        current_frame, reward, done, _ = env.step(action)\n",
    "\n",
    "        current_frame = prepro(current_frame)\n",
    "\n",
    "        if done:\n",
    "            episode_num += 1\n",
    "            previous_frame = None\n",
    "            current_frame = prepro(env.reset())\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    print(\"Finished!!!\")\n",
    "\n",
    "\n",
    "play_game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
